{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. mount gdrive first / run the following\n",
        "# !pip install gdown\n",
        "# !gdown https://drive.google.com/uc?id=<file_id>\n",
        "# !gdown --folder https://drive.google.com/drive/folders/<file_id>\n"
      ],
      "metadata": {
        "id": "JiQxLFOItD-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Download dataset/MuseMorphose-main.zip and unzip\n",
        "# if in Colab, mounted gdrive:\n",
        "!unzip \"/content/drive/MyDrive/Capstone/dataset/MuseMorphose-main.zip\" -d \"./\""
      ],
      "metadata": {
        "id": "J9QOC3Nug_ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EpLhsfWsUsw"
      },
      "outputs": [],
      "source": [
        "# # an example of musemorphose's data\n",
        "# # download REMI-pop-1.7K dataset\n",
        "# !wget -O remi_dataset.tar.gz https://zenodo.org/record/4782721/files/remi_dataset.tar.gz?download=1\n",
        "# !tar xzvf remi_dataset.tar.gz\n",
        "# !rm remi_dataset.tar.gz\n",
        "\n",
        "# # compute attributes classes\n",
        "# !python3 attributes.py\n",
        "\n",
        "# rename MuseMorphose dataset\n",
        "# !mv /content/MuseMorphose-main/remi_dataset /content/MuseMorphose-main/remi_dataset_mm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. untar our data located in dataset/remi_dataset_0907_updated.tar.gz (if in Colab, mounted gdrive:)\n",
        "!tar xzvf /content/drive/MyDrive/Capstone/dataset/remi_dataset_0907_updated.tar.gz"
      ],
      "metadata": {
        "id": "6Kg60yj5srT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. move our dataset to the folder\n",
        "!mv /content/content/remi_dataset /content/MuseMorphose-main/remi_dataset"
      ],
      "metadata": {
        "id": "sorlCpa4vXgJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Musemorphose remi_vocab\n",
        "# pickle_load(\"/content/MuseMorphose-main/pickles/remi_vocab.pkl\")"
      ],
      "metadata": {
        "id": "o8DOfqhBwx_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle_load(\"/content/content/compound-word-transformer-main/dataset/representations/uncond/remi/ailab17k_from-scratch_remi/dictionary.pkl\")"
      ],
      "metadata": {
        "id": "l5iH4EmkyExv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. rename/remove the original remi_vocab located in MuseMorphose-main/pickles/remi_vocab.pkl \n",
        "!mv /content/MuseMorphose-main/pickles/remi_vocab.pkl /content/MuseMorphose-main/pickles/remi_vocab_mm.pkl"
      ],
      "metadata": {
        "id": "jdW-xctNyNyl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. add our remi_vocab, located in dataset/remi_vocab.pkl\n",
        "!cp \"/content/drive/MyDrive/Capstone/Deep Music Generation Group Folder/dataset/remi_vocab.pkl\" \"/content/MuseMorphose-main/pickles/\""
      ],
      "metadata": {
        "id": "x_RbWyvPyVla"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. obtain label files\n",
        "# located in shared drive /dataset/remi_splitted/\n",
        "import csv\n",
        "\n",
        "def print_dict_ex(d):\n",
        "  print(\"Printing 3 lines of examples ...\")\n",
        "  count = 0\n",
        "  for k, v in d.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "    count += 1\n",
        "    if count >= 3:\n",
        "      break\n",
        "\n",
        "def load_tuples(fn):\n",
        "  fn_map = {}\n",
        "  recording_track_map = {}\n",
        "  with open(fn) as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    count = 0\n",
        "    for row in csv_reader:\n",
        "      assert len(row) == 2\n",
        "      if count == 0:\n",
        "        print(row)\n",
        "      else:\n",
        "        track, recording, chunk = row[0].split(\"_\")\n",
        "        map_id = f\"{recording}_{chunk}\"\n",
        "        if map_id in fn_map:\n",
        "          fn_map[map_id].append(row[1])\n",
        "        else:\n",
        "          fn_map[map_id] = [row[1]]\n",
        "      count += 1\n",
        "    print(f\"Processed {count - 1} lines of tuples.\")\n",
        "  return fn_map\n",
        "\n",
        "neg_test = load_tuples(\"/content/drive/MyDrive/Capstone/Deep Music Generation Group Folder/dataset/remi_splitted/neg_tuples_test.csv\")\n",
        "neg_train = load_tuples(\"/content/drive/MyDrive/Capstone/Deep Music Generation Group Folder/dataset/remi_splitted/neg_tuples_train.csv\")\n",
        "neg_val = load_tuples(\"/content/drive/MyDrive/Capstone/Deep Music Generation Group Folder/dataset/remi_splitted/neg_tuples_val.csv\")\n",
        "pos_test = load_tuples(\"/content/drive/MyDrive/Capstone/Deep Music Generation Group Folder/dataset/remi_splitted/pos_tuples_test.csv\")\n",
        "pos_train = load_tuples(\"/content/drive/MyDrive/Capstone/Deep Music Generation Group Folder/dataset/remi_splitted/pos_tuples_train.csv\")\n",
        "pos_val = load_tuples(\"/content/drive/MyDrive/Capstone/Deep Music Generation Group Folder/dataset/remi_splitted/pos_tuples_val.csv\")\n",
        "print_dict_ex(neg_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU2-btVhVsf4",
        "outputId": "2b5e4e80-bcc3-4440-ea18-1c2f00e2a606"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['file_name', 'label']\n",
            "Processed 63240 lines of tuples.\n",
            "['file_name', 'label']\n",
            "Processed 574998 lines of tuples.\n",
            "['file_name', 'label']\n",
            "Processed 80007 lines of tuples.\n",
            "['file_name', 'label']\n",
            "Processed 63240 lines of tuples.\n",
            "['file_name', 'label']\n",
            "Processed 574998 lines of tuples.\n",
            "['file_name', 'label']\n",
            "Processed 80007 lines of tuples.\n",
            "Printing 3 lines of examples ...\n",
            "23b932508e50b703a605bffda6324c01_0: ['Doo Wop', 'I can honestly say that I believe &quot;Dirt&quot; is a masterpiece.Forget the grunge stereotype, this is a brutal beauty.My favorite tracks are &quot;Them Bones&quot; &quot;Rooster&quot; and &quot;Sickman&quot;.  Talk  about a wall of sound !']\n",
            "23b932508e50b703a605bffda6324c01_1: ['1979: the fag end of punk was dwindling to a gradual stop as Britain, it seemed, reached a cataclysmic collapse.  For those not in the UK during the infamous \"winter of discontent\" of 1978-9, Britain was on the brink of meltdown as everyone went on strike, the bins went unemptied, the dead unburied.  So what gave people hope during this bleak time?  DISCO!  Clashing head-on with the grubby ethics of guitar rock, disco dared to beebullient, hedonistic and escapist.  It was a Marshall Plan of a musical movement.  As with most popular musics, it was to leave a lasting legacy; one that is still celebrated now.Britain (and Europe generally) engaged completely with (mostly American) disco in the wake of Saturday Night Fever.  Arguably there are fewer thantwenty really solid gold disco classics but \\'Beat The Clock\\' is one of them. Blown away by what Moroder had done two years previously with the sublime \\'I Feel Love\\', the Maels approached him to produce their next album.  Junking the traditional guitars/drums approach, this is the Sparks album where Ron and Giorgio fully availed themselves of the synth technology of the time.  Moroder hadn\\'t worked with a band since Chichory (Son Of My Father) Tip some years before and the Maels had never worked with someone like him before and the combination was dynamite.  I was 13 years old and totally destroyed by this album.As with Kraftwerk\\'s classic album from the previous year, \\'The Man Machine\\', it has just six tracks but all of them are real winners.  The lyrics, the sounds, the empassioned drumming (a medal to whoever drummed on the songs!), the production and everything else meshed together to produce an incredible album which was both of the time and ahead of it.Stand-out tracks?  The three singles but especially \\'The Number One Song In Heaven\\' because it\\'s just so complete.  If you only buy one album this year, make it this one.Ps point of correction: the Yamaha DX7 synth wasn\\'t released until 1983.  Nearly all of \\'Number One In Heaven\\' was conceived on ARP and Roland modular synths.', 'British Folk']\n",
            "23b932508e50b703a605bffda6324c01_2: [\"Another master CD on vocal performance and piano playing. One of the live dates by Torm&eacute; and Shearing for Concord Records. Please don't count on anyone serious to choose only one of these CDs. Buy all. This CD specially  offers a sensational medley based on &quot;New York, New York&quot; where  all songs are played backed by the same rhythm bars which opens &quot;New  York ...&quot;. Great, Intelligent, Creative.\", 'Electronic Pop']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MuseMorphose-main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz_08Dbrv0Kc",
        "outputId": "206c709d-590d-4357-eb65-1e28527422a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MuseMorphose-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "YNtQoYKpsrgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pickle, random\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "IDX_TO_KEY = {\n",
        "  0: 'A',\n",
        "  1: 'A#',\n",
        "  2: 'B',\n",
        "  3: 'C',\n",
        "  4: 'C#',\n",
        "  5: 'D',\n",
        "  6: 'D#',\n",
        "  7: 'E',\n",
        "  8: 'F',\n",
        "  9: 'F#',\n",
        "  10: 'G',\n",
        "  11: 'G#'\n",
        "}\n",
        "KEY_TO_IDX = {\n",
        "  v:k for k, v in IDX_TO_KEY.items()\n",
        "}\n"
      ],
      "metadata": {
        "id": "FjUfdsnRsgu4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chord_tone(chord_event):\n",
        "  tone = chord_event['value'].split('_')[0]\n",
        "  return tone\n",
        "\n",
        "def transpose_chord(chord_event, n_keys):\n",
        "  if chord_event['value'] == 'N_N':\n",
        "    return chord_event\n",
        "\n",
        "  orig_tone = get_chord_tone(chord_event)\n",
        "  orig_tone_idx = KEY_TO_IDX[orig_tone]\n",
        "  new_tone_idx = (orig_tone_idx + 12 + n_keys) % 12\n",
        "  new_chord_value = chord_event['value'].replace(\n",
        "    '{}_'.format(orig_tone), '{}_'.format(IDX_TO_KEY[new_tone_idx])\n",
        "  )\n",
        "  new_chord_event = {'name': chord_event['name'], 'value': new_chord_value}\n",
        "  # print ('keys={}. {} --> {}'.format(n_keys, chord_event, new_chord_event))\n",
        "\n",
        "  return new_chord_event"
      ],
      "metadata": {
        "id": "uHgAy8Z0tVO8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_extreme_pitch(raw_events):\n",
        "  low, high = 128, 0\n",
        "  for ev in raw_events:\n",
        "    if ev['name'] == 'Note_Pitch':\n",
        "      low = min(low, int(ev['value']))\n",
        "      high = max(high, int(ev['value']))\n",
        "\n",
        "  return low, high\n",
        "\n",
        "def transpose_events(raw_events, n_keys):\n",
        "  transposed_raw_events = []\n",
        "\n",
        "  for ev in raw_events:\n",
        "    if ev['name'] == 'Note_Pitch':\n",
        "      transposed_raw_events.append(\n",
        "        {'name': ev['name'], 'value': ev['value'] + n_keys}\n",
        "      )\n",
        "    elif ev['name'] == 'Chord':\n",
        "      transposed_raw_events.append(\n",
        "        transpose_chord(ev, n_keys)\n",
        "      )\n",
        "    else:\n",
        "      transposed_raw_events.append(ev)\n",
        "\n",
        "  assert len(transposed_raw_events) == len(raw_events)\n",
        "  return transposed_raw_events"
      ],
      "metadata": {
        "id": "OospaoUKtZmd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pickle_load(path):\n",
        "  return pickle.load(open(path, 'rb'))\n",
        "\n",
        "def convert_event(event_seq, event2idx, to_ndarr=True):\n",
        "  if isinstance(event_seq[0], dict):\n",
        "    event_seq = [event2idx['{}_{}'.format(e['name'], e['value'])] for e in event_seq]\n",
        "  else:\n",
        "    event_seq = [event2idx[e] for e in event_seq]\n",
        "\n",
        "  if to_ndarr:\n",
        "    return np.array(event_seq)\n",
        "  else:\n",
        "    return event_seq"
      ],
      "metadata": {
        "id": "RBo28jmWtb8N"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd.grad_mode import F\n",
        "class REMIFullSongTransformerDataset(Dataset):\n",
        "  def __init__(self, data_dir, vocab_file, \n",
        "               model_enc_seqlen=128, model_dec_seqlen=1280, model_max_bars=16,\n",
        "               pieces=[], do_augment=True, augment_range=range(-6, 7), \n",
        "               min_pitch=22, max_pitch=107, pad_to_same=True, use_attr_cls=True,\n",
        "               appoint_st_bar=None, dec_end_pad_value=None, pos_map=None, neg_map=None):\n",
        "    self.vocab_file = vocab_file\n",
        "    self.read_vocab()\n",
        "\n",
        "    self.data_dir = data_dir\n",
        "    self.pieces = pieces\n",
        "    self.build_dataset()\n",
        "\n",
        "    self.model_enc_seqlen = model_enc_seqlen\n",
        "    self.model_dec_seqlen = model_dec_seqlen\n",
        "    self.model_max_bars = model_max_bars\n",
        "\n",
        "    self.do_augment = do_augment\n",
        "    self.augment_range = augment_range\n",
        "    self.min_pitch, self.max_pitch = min_pitch, max_pitch\n",
        "    self.pad_to_same = pad_to_same\n",
        "    self.use_attr_cls = use_attr_cls\n",
        "\n",
        "    self.appoint_st_bar = appoint_st_bar\n",
        "    if dec_end_pad_value is None:\n",
        "      self.dec_end_pad_value = self.pad_token\n",
        "    elif dec_end_pad_value == 'EOS':\n",
        "      self.dec_end_pad_value = self.eos_token\n",
        "    else:\n",
        "      self.dec_end_pad_value = self.pad_token\n",
        "\n",
        "    # added for our labels\n",
        "    if pos_map is None or neg_map is None:\n",
        "      raise ValueError(\"pos_map and neg_map cannot be None.\")\n",
        "    self.matched_pieces = self.match_labels(pos_map, neg_map)\n",
        "\n",
        "  def match_labels(self, pos_map, neg_map):\n",
        "    # match fn with different pos/neg labels\n",
        "    data_groups = []\n",
        "    for idx in range(len(self.pieces)):\n",
        "      f = self.pieces[idx]\n",
        "      fn = f.split(\"/\")[-1].split(\".\")[0]\n",
        "      if fn in pos_map:\n",
        "        for label in pos_map[fn]:\n",
        "          pos = True\n",
        "          data_groups.append((idx, fn, pos, label))\n",
        "      if fn in neg_map:\n",
        "        for label in neg_map[fn]:\n",
        "          pos = False\n",
        "          data_groups.append((idx, fn, pos, label))\n",
        "    return data_groups\n",
        "\n",
        "  def read_vocab(self):\n",
        "    vocab = pickle_load(self.vocab_file)[0]\n",
        "    self.idx2event = pickle_load(self.vocab_file)[1]\n",
        "    orig_vocab_size = len(vocab)\n",
        "    self.event2idx = vocab\n",
        "    self.bar_token = self.event2idx['Bar_None']\n",
        "    self.eos_token = self.event2idx['EOS_None']\n",
        "    self.pad_token = orig_vocab_size\n",
        "    self.vocab_size = self.pad_token + 1\n",
        "  \n",
        "  def build_dataset(self):\n",
        "    if not self.pieces:\n",
        "      self.pieces = sorted( glob(os.path.join(self.data_dir, '*.pkl')) )\n",
        "    else:\n",
        "      self.pieces = sorted( [os.path.join(self.data_dir, p) for p in self.pieces] )\n",
        "\n",
        "    self.piece_bar_pos = []\n",
        "\n",
        "    for i, p in enumerate(self.pieces):\n",
        "      bar_pos, p_evs = pickle_load(p)\n",
        "      if not i % 200:\n",
        "        print ('[preparing data] now at #{}'.format(i))\n",
        "      if bar_pos[-1] == len(p_evs):\n",
        "        print ('piece {}, got appended bar markers'.format(p))\n",
        "        bar_pos = bar_pos[:-1]\n",
        "      if len(p_evs) - bar_pos[-1] == 2:\n",
        "        # got empty trailing bar\n",
        "        bar_pos = bar_pos[:-1]\n",
        "\n",
        "      bar_pos.append(len(p_evs))\n",
        "\n",
        "      self.piece_bar_pos.append(bar_pos)\n",
        "\n",
        "  def get_sample_from_file(self, piece_idx):\n",
        "    piece_evs = pickle_load(self.pieces[piece_idx])[1]\n",
        "    if len(self.piece_bar_pos[piece_idx]) > self.model_max_bars and self.appoint_st_bar is None:\n",
        "      picked_st_bar = random.choice(\n",
        "        range(len(self.piece_bar_pos[piece_idx]) - self.model_max_bars)\n",
        "      )\n",
        "    elif self.appoint_st_bar is not None and self.appoint_st_bar < len(self.piece_bar_pos[piece_idx]) - self.model_max_bars:\n",
        "      picked_st_bar = self.appoint_st_bar\n",
        "    else:\n",
        "      picked_st_bar = 0\n",
        "\n",
        "    piece_bar_pos = self.piece_bar_pos[piece_idx]\n",
        "\n",
        "    if len(piece_bar_pos) > self.model_max_bars:\n",
        "      piece_evs = piece_evs[ piece_bar_pos[picked_st_bar] : piece_bar_pos[picked_st_bar + self.model_max_bars] ]\n",
        "      picked_bar_pos = np.array(piece_bar_pos[ picked_st_bar : picked_st_bar + self.model_max_bars ]) - piece_bar_pos[picked_st_bar]\n",
        "      n_bars = self.model_max_bars\n",
        "    else:\n",
        "      picked_bar_pos = np.array(piece_bar_pos + [piece_bar_pos[-1]] * (self.model_max_bars - len(piece_bar_pos)))\n",
        "      n_bars = len(piece_bar_pos)\n",
        "      assert len(picked_bar_pos) == self.model_max_bars\n",
        "\n",
        "    return piece_evs, picked_st_bar, picked_bar_pos, n_bars\n",
        "\n",
        "  def pad_sequence(self, seq, maxlen, pad_value=None):\n",
        "    if pad_value is None:\n",
        "      pad_value = self.pad_token\n",
        "\n",
        "    seq.extend( [pad_value for _ in range(maxlen- len(seq))] )\n",
        "\n",
        "    return seq\n",
        "\n",
        "  def pitch_augment(self, bar_events):\n",
        "    bar_min_pitch, bar_max_pitch = check_extreme_pitch(bar_events)\n",
        "    \n",
        "    n_keys = random.choice(self.augment_range)\n",
        "    while bar_min_pitch + n_keys < self.min_pitch or bar_max_pitch + n_keys > self.max_pitch:\n",
        "      n_keys = random.choice(self.augment_range)\n",
        "\n",
        "    augmented_bar_events = transpose_events(bar_events, n_keys)\n",
        "    return augmented_bar_events\n",
        "\n",
        "  def get_attr_classes(self, piece, st_bar):\n",
        "    polyph_cls = pickle_load(os.path.join(self.data_dir, 'attr_cls/polyph', piece))[st_bar : st_bar + self.model_max_bars]\n",
        "    rfreq_cls = pickle_load(os.path.join(self.data_dir, 'attr_cls/rhythm', piece))[st_bar : st_bar + self.model_max_bars]\n",
        "\n",
        "    polyph_cls.extend([0 for _ in range(self.model_max_bars - len(polyph_cls))])\n",
        "    rfreq_cls.extend([0 for _ in range(self.model_max_bars - len(rfreq_cls))])\n",
        "\n",
        "    assert len(polyph_cls) == self.model_max_bars\n",
        "    assert len(rfreq_cls) == self.model_max_bars\n",
        "\n",
        "    return polyph_cls, rfreq_cls\n",
        "\n",
        "  def get_encoder_input_data(self, bar_positions, bar_events):\n",
        "    assert len(bar_positions) == self.model_max_bars + 1\n",
        "    enc_padding_mask = np.ones((self.model_max_bars, self.model_enc_seqlen), dtype=bool)\n",
        "    enc_padding_mask[:, :2] = False\n",
        "    padded_enc_input = np.full((self.model_max_bars, self.model_enc_seqlen), dtype=int, fill_value=self.pad_token)\n",
        "    enc_lens = np.zeros((self.model_max_bars,))\n",
        "\n",
        "    for b, (st, ed) in enumerate(zip(bar_positions[:-1], bar_positions[1:])):\n",
        "      enc_padding_mask[b, : (ed-st)] = False\n",
        "      enc_lens[b] = ed - st\n",
        "      within_bar_events = self.pad_sequence(bar_events[st : ed], self.model_enc_seqlen, self.pad_token)\n",
        "      within_bar_events = np.array(within_bar_events)\n",
        "\n",
        "      padded_enc_input[b, :] = within_bar_events[:self.model_enc_seqlen]\n",
        "\n",
        "    return padded_enc_input, enc_padding_mask, enc_lens\n",
        "\n",
        "  def __len__(self):\n",
        "    # return len(self.pieces)\n",
        "    return len(self.matched_pieces)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      print(\"idx\", idx)\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    # original idx for indexing self.pieces\n",
        "    # now we need to idx self.matched_pieces\n",
        "    # get original idx and matched idx\n",
        "\n",
        "    matched_idx = idx\n",
        "    matched_data = self.matched_pieces[idx]\n",
        "    idx = matched_data[0]\n",
        "    text_labels = matched_data[3]\n",
        "    pos = matched_data[2]\n",
        "\n",
        "    bar_events, st_bar, bar_pos, enc_n_bars = self.get_sample_from_file(idx)\n",
        "    if self.do_augment:\n",
        "      bar_events = self.pitch_augment(bar_events)\n",
        "\n",
        "    if self.use_attr_cls:\n",
        "      polyph_cls, rfreq_cls = self.get_attr_classes(os.path.basename(self.pieces[idx]), st_bar)\n",
        "      polyph_cls_expanded = np.zeros((self.model_dec_seqlen,), dtype=int)\n",
        "      rfreq_cls_expanded = np.zeros((self.model_dec_seqlen,), dtype=int)\n",
        "      for i, (b_st, b_ed) in enumerate(zip(bar_pos[:-1], bar_pos[1:])):\n",
        "        polyph_cls_expanded[b_st:b_ed] = polyph_cls[i]\n",
        "        rfreq_cls_expanded[b_st:b_ed] = rfreq_cls[i]\n",
        "    else:\n",
        "      polyph_cls, rfreq_cls = [0], [0]\n",
        "      polyph_cls_expanded, rfreq_cls_expanded = [0], [0]\n",
        "\n",
        "    bar_tokens = convert_event(bar_events, self.event2idx, to_ndarr=False)\n",
        "    bar_pos = bar_pos.tolist() + [len(bar_tokens)]\n",
        "\n",
        "    enc_inp, enc_padding_mask, enc_lens = self.get_encoder_input_data(bar_pos, bar_tokens)\n",
        "\n",
        "    length = len(bar_tokens)\n",
        "    if self.pad_to_same:\n",
        "      inp = self.pad_sequence(bar_tokens, self.model_dec_seqlen + 1) \n",
        "    else:\n",
        "      inp = self.pad_sequence(bar_tokens, len(bar_tokens) + 1, pad_value=self.dec_end_pad_value)\n",
        "    target = np.array(inp[1:], dtype=int)\n",
        "    inp = np.array(inp[:-1], dtype=int)\n",
        "    assert len(inp) == len(target)\n",
        "\n",
        "    return {\n",
        "      'id': idx,\n",
        "      # 'piece_id': int(os.path.basename(self.pieces[idx]).replace('.pkl', '')),\n",
        "      'piece_id': os.path.basename(self.pieces[idx]).replace('.pkl', ''),\n",
        "      'st_bar_id': st_bar,\n",
        "      'bar_pos': np.array(bar_pos, dtype=int),\n",
        "      'enc_input': enc_inp,\n",
        "      'dec_input': inp[:self.model_dec_seqlen],\n",
        "      'dec_target': target[:self.model_dec_seqlen],\n",
        "      'polyph_cls': polyph_cls_expanded,\n",
        "      'rhymfreq_cls': rfreq_cls_expanded,\n",
        "      'polyph_cls_bar': np.array(polyph_cls),\n",
        "      'rhymfreq_cls_bar': np.array(rfreq_cls),\n",
        "      'length': min(length, self.model_dec_seqlen),\n",
        "      'enc_padding_mask': enc_padding_mask,\n",
        "      'enc_length': enc_lens,\n",
        "      'enc_n_bars': enc_n_bars,\n",
        "      'text_label': text_labels,\n",
        "      'pos': pos,  # whether this is a positive tuple\n",
        "    }"
      ],
      "metadata": {
        "id": "n_R6AAYntd5n"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change pos_map and neg_map to generate val_dset, test_dset\n",
        "train_dset = REMIFullSongTransformerDataset(\n",
        "  './remi_dataset', './pickles/remi_vocab.pkl', do_augment=True, use_attr_cls=True,\n",
        "  model_max_bars=16, model_dec_seqlen=1280, model_enc_seqlen=128, min_pitch=22, max_pitch=107,\n",
        "  pos_map=pos_train, neg_map=neg_train,\n",
        ")\n",
        "print (train_dset.bar_token, train_dset.pad_token, train_dset.vocab_size)\n",
        "print ('length:', len(train_dset))\n",
        "\n",
        "# for i in random.sample(range(len(dset)), 100):\n",
        "# for i in range(len(dset)):\n",
        "#   sample = dset[i]\n",
        "  # print (i, len(sample['bar_pos']), sample['bar_pos'])\n",
        "  # print (i)\n",
        "  # print ('******* ----------- *******')\n",
        "  # print ('piece: {}, st_bar: {}'.format(sample['piece_id'], sample['st_bar_id']))\n",
        "  # print (sample['enc_input'][:8, :16])\n",
        "  # print (sample['dec_input'][:16])\n",
        "  # print (sample['dec_target'][:16])\n",
        "  # print (sample['enc_padding_mask'][:32, :16])\n",
        "  # print (sample['length'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rnh8gypstjop",
        "outputId": "772b5e40-f758-47be-f085-5db318806724"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[preparing data] now at #0\n",
            "[preparing data] now at #200\n",
            "[preparing data] now at #400\n",
            "[preparing data] now at #600\n",
            "[preparing data] now at #800\n",
            "[preparing data] now at #1000\n",
            "[preparing data] now at #1200\n",
            "[preparing data] now at #1400\n",
            "[preparing data] now at #1600\n",
            "[preparing data] now at #1800\n",
            "[preparing data] now at #2000\n",
            "[preparing data] now at #2200\n",
            "[preparing data] now at #2400\n",
            "[preparing data] now at #2600\n",
            "[preparing data] now at #2800\n",
            "[preparing data] now at #3000\n",
            "[preparing data] now at #3200\n",
            "[preparing data] now at #3400\n",
            "[preparing data] now at #3600\n",
            "[preparing data] now at #3800\n",
            "[preparing data] now at #4000\n",
            "[preparing data] now at #4200\n",
            "[preparing data] now at #4400\n",
            "[preparing data] now at #4600\n",
            "[preparing data] now at #4800\n",
            "[preparing data] now at #5000\n",
            "[preparing data] now at #5200\n",
            "[preparing data] now at #5400\n",
            "[preparing data] now at #5600\n",
            "[preparing data] now at #5800\n",
            "[preparing data] now at #6000\n",
            "[preparing data] now at #6200\n",
            "[preparing data] now at #6400\n",
            "[preparing data] now at #6600\n",
            "[preparing data] now at #6800\n",
            "[preparing data] now at #7000\n",
            "[preparing data] now at #7200\n",
            "[preparing data] now at #7400\n",
            "[preparing data] now at #7600\n",
            "[preparing data] now at #7800\n",
            "[preparing data] now at #8000\n",
            "[preparing data] now at #8200\n",
            "[preparing data] now at #8400\n",
            "[preparing data] now at #8600\n",
            "[preparing data] now at #8800\n",
            "[preparing data] now at #9000\n",
            "[preparing data] now at #9200\n",
            "[preparing data] now at #9400\n",
            "[preparing data] now at #9600\n",
            "[preparing data] now at #9800\n",
            "[preparing data] now at #10000\n",
            "[preparing data] now at #10200\n",
            "[preparing data] now at #10400\n",
            "[preparing data] now at #10600\n",
            "[preparing data] now at #10800\n",
            "[preparing data] now at #11000\n",
            "[preparing data] now at #11200\n",
            "[preparing data] now at #11400\n",
            "[preparing data] now at #11600\n",
            "[preparing data] now at #11800\n",
            "[preparing data] now at #12000\n",
            "[preparing data] now at #12200\n",
            "[preparing data] now at #12400\n",
            "[preparing data] now at #12600\n",
            "[preparing data] now at #12800\n",
            "[preparing data] now at #13000\n",
            "[preparing data] now at #13200\n",
            "[preparing data] now at #13400\n",
            "[preparing data] now at #13600\n",
            "[preparing data] now at #13800\n",
            "[preparing data] now at #14000\n",
            "[preparing data] now at #14200\n",
            "[preparing data] now at #14400\n",
            "[preparing data] now at #14600\n",
            "[preparing data] now at #14800\n",
            "[preparing data] now at #15000\n",
            "[preparing data] now at #15200\n",
            "[preparing data] now at #15400\n",
            "[preparing data] now at #15600\n",
            "[preparing data] now at #15800\n",
            "[preparing data] now at #16000\n",
            "[preparing data] now at #16200\n",
            "[preparing data] now at #16400\n",
            "[preparing data] now at #16600\n",
            "[preparing data] now at #16800\n",
            "[preparing data] now at #17000\n",
            "[preparing data] now at #17200\n",
            "[preparing data] now at #17400\n",
            "[preparing data] now at #17600\n",
            "[preparing data] now at #17800\n",
            "[preparing data] now at #18000\n",
            "[preparing data] now at #18200\n",
            "[preparing data] now at #18400\n",
            "[preparing data] now at #18600\n",
            "[preparing data] now at #18800\n",
            "[preparing data] now at #19000\n",
            "[preparing data] now at #19200\n",
            "[preparing data] now at #19400\n",
            "[preparing data] now at #19600\n",
            "[preparing data] now at #19800\n",
            "[preparing data] now at #20000\n",
            "[preparing data] now at #20200\n",
            "[preparing data] now at #20400\n",
            "[preparing data] now at #20600\n",
            "[preparing data] now at #20800\n",
            "[preparing data] now at #21000\n",
            "[preparing data] now at #21200\n",
            "[preparing data] now at #21400\n",
            "[preparing data] now at #21600\n",
            "[preparing data] now at #21800\n",
            "[preparing data] now at #22000\n",
            "[preparing data] now at #22200\n",
            "[preparing data] now at #22400\n",
            "[preparing data] now at #22600\n",
            "[preparing data] now at #22800\n",
            "[preparing data] now at #23000\n",
            "[preparing data] now at #23200\n",
            "[preparing data] now at #23400\n",
            "[preparing data] now at #23600\n",
            "[preparing data] now at #23800\n",
            "[preparing data] now at #24000\n",
            "[preparing data] now at #24200\n",
            "[preparing data] now at #24400\n",
            "[preparing data] now at #24600\n",
            "[preparing data] now at #24800\n",
            "[preparing data] now at #25000\n",
            "[preparing data] now at #25200\n",
            "[preparing data] now at #25400\n",
            "[preparing data] now at #25600\n",
            "[preparing data] now at #25800\n",
            "[preparing data] now at #26000\n",
            "[preparing data] now at #26200\n",
            "[preparing data] now at #26400\n",
            "[preparing data] now at #26600\n",
            "[preparing data] now at #26800\n",
            "[preparing data] now at #27000\n",
            "[preparing data] now at #27200\n",
            "[preparing data] now at #27400\n",
            "[preparing data] now at #27600\n",
            "[preparing data] now at #27800\n",
            "[preparing data] now at #28000\n",
            "[preparing data] now at #28200\n",
            "[preparing data] now at #28400\n",
            "[preparing data] now at #28600\n",
            "[preparing data] now at #28800\n",
            "[preparing data] now at #29000\n",
            "[preparing data] now at #29200\n",
            "[preparing data] now at #29400\n",
            "[preparing data] now at #29600\n",
            "[preparing data] now at #29800\n",
            "[preparing data] now at #30000\n",
            "[preparing data] now at #30200\n",
            "[preparing data] now at #30400\n",
            "[preparing data] now at #30600\n",
            "[preparing data] now at #30800\n",
            "[preparing data] now at #31000\n",
            "[preparing data] now at #31200\n",
            "[preparing data] now at #31400\n",
            "[preparing data] now at #31600\n",
            "[preparing data] now at #31800\n",
            "[preparing data] now at #32000\n",
            "[preparing data] now at #32200\n",
            "[preparing data] now at #32400\n",
            "[preparing data] now at #32600\n",
            "[preparing data] now at #32800\n",
            "[preparing data] now at #33000\n",
            "[preparing data] now at #33200\n",
            "[preparing data] now at #33400\n",
            "[preparing data] now at #33600\n",
            "[preparing data] now at #33800\n",
            "[preparing data] now at #34000\n",
            "[preparing data] now at #34200\n",
            "0 403 404\n",
            "length: 1149996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dloader = DataLoader(train_dset, batch_size=4, shuffle=False, num_workers=24)\n",
        "for i, batch in enumerate(train_dloader):\n",
        "  for k, v in batch.items():\n",
        "    if torch.is_tensor(v):\n",
        "      print (k, ':', v.dtype, v.size())\n",
        "    else:\n",
        "      print(k, ':', v)\n",
        "  break\n",
        "  print ('=====================================\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QkROl4CwrUf",
        "outputId": "18e1ce26-532d-40eb-9574-2cdf963a052a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id : torch.int64 torch.Size([4])\n",
            "piece_id : ['000869391f4fc43161b1d30676cb3fa8_0', '000869391f4fc43161b1d30676cb3fa8_0', '000869391f4fc43161b1d30676cb3fa8_0', '000869391f4fc43161b1d30676cb3fa8_0']\n",
            "st_bar_id : torch.int64 torch.Size([4])\n",
            "bar_pos : torch.int64 torch.Size([4, 17])\n",
            "enc_input : torch.int64 torch.Size([4, 16, 128])\n",
            "dec_input : torch.int64 torch.Size([4, 1280])\n",
            "dec_target : torch.int64 torch.Size([4, 1280])\n",
            "polyph_cls : torch.int64 torch.Size([4, 1280])\n",
            "rhymfreq_cls : torch.int64 torch.Size([4, 1280])\n",
            "polyph_cls_bar : torch.int64 torch.Size([4, 16])\n",
            "rhymfreq_cls_bar : torch.int64 torch.Size([4, 16])\n",
            "length : torch.int64 torch.Size([4])\n",
            "enc_padding_mask : torch.bool torch.Size([4, 16, 128])\n",
            "enc_length : torch.float64 torch.Size([4, 16])\n",
            "enc_n_bars : torch.int64 torch.Size([4])\n",
            "text_label : [\"'B*Witched' is my first pop album I brought, and I am impressed. Unlike other pop albums, this one has a strong cultural feel in most of the songs. This is what makes the album so original and refreshing. The songs also  reflect a sense of happinness and enjoyment, encouraging me to play the  album again and again. Good work B*Witched!\", \"1. Let's Go (The B*Witched Jig) - this song has no lyrics...well, besides at the verrry beginning, but it's no singing. i enjoy the beat and the irish music to it. every though it's an instrumental, it's definetally something you have to get up and dance to. - 8.9/102. C'est La Vie - this is one of my favorite songs on the cd. the lyrics are very,very cute and i LOVE the music! It's mostly pop-ish through it all until the end and then it has bagpipe...really awesome. ^^ - 10/103. Rev It Up - i really enjoy rev it up's chorus. edele's voice is so beautiful in this song. - 9/104. To You I Belong - slow song...BEAUTIFUL song...there isnt much to say about it. i love the lyrics (as i do almost all of them). they're really sad, and all their voices just mix together where it almost makes you want to cry. *tear,tear* - 10/105. Rollercoaster - very fast, exciting song. something to dance to and sing to. the lyrics are AWESOME! i guarantee anyone who likes music in general will love this song. - 9.5/106. Blame It On The Weatherman  - blame it on the weatherman is about a girl that goes to meet this guy and he never shows. very slow song, and really sad. there's a remix of it on the second cd, i think this one is alot better. love all the instruments!! - 10/107. We Four Girls - this song is fast and spunky! the lyrics are a little weird, though. 'we four girl are here to stay! come on, yeah, yeah' and that's ALMOST all they say. but still, great beat and great voices. - 8.9/108. Castles In The Air  - i LOVE this song!! it's my second favorite on the cd, probably. a little slow-not too slow, but just perfect. i think it's just beautiful...not too sure how to explain it. - 10/109. Freak Out - this was one of the very few songs b-witched didnt write, but it's still awesome! this is one of the fastest songs on the cd and will make anyone dance to it. cute lyrics, too! - 9.7/1010. Like The Rose - this is my FAVORITE SONG!!! it's sooo beautiful! the lyrics are so beautiful they make me almost wanna cry...!...yea, i'm emmotional. =P heh, but the music is slow and edele, keavy, lindsay, and sinead's voices counldn't sound anymore beautiful. - 10/1011. Never Giving Up - i love the music and the beat in this song. the lyrics are cute, almost like something you'd say to your first love...i really enjoy this song also. - 8.5/1012. Oh Mr. Postman - most people may think this song is a little weird because of the name and all, but it's really awesome!! the lyrics are about this girl that's waiting for a letter from her boyfriend. 'Oh Mr. Postman give me a sign, tell me you've a letter to make me feel fine...' yup, it's a slower song, and it really pertyful. ^^ - 9.8/10OVERALL: 9.5/10I think this cd is very worth buying!! as i said in the title. this is a cd of irish-pop music that nobody can resist...GO BUY IT! NOW!\", \"1.) Let's Go (B*Witched Jig) : 5/52.) C'est La Vie: 5/5 *their 1st single,very nice and energetic3.) Rev it Up: 5/54.) To You I Belong: 5/55.) Rollercoaster: 5/5 *my fav song on the album6.) Blame it on the Wheatherman: 5/57.) We Four Girls: 2/5 *my least fav8.) Castle in the Air: 5/5 *my fav slow song9.) Freak Out: 5/5 *nice dance song10.) Like A Rose: 5/511.) Never Giving  Up: 5/512.) Oh Mr. Postman: 5/5\", \"1.) Let's Go - This is just an Irish jig. Gets you in the mood.2.) C'est la vie - I heard this song once and I loved it. This one is great to listen to while cleaning your room, because because it sounds so upbeat and  happy.3.) Rev it up - Nice song, worth the listen.4.) To you I belong  - Sudden change from C'est la vie, this is slower. This is also a nice  song.5.) Rollercoaster - My second fave after C'est la vie!  Rollercoaster just sends you singing and dancing!6.) Blame it on the  Weatherman - Don't like this song very much, I'm more into the upbeat and  happy songs.7.) We Four Girls - I don't like this very much either, it's  not really my taste, I guess. Too...I dunno, loud? It basically repeats  over and over that &quot;We Four Girls are here to stay&quot;. Well, I got  the message!8.) Castles in the air - This is a sudden change from We  Four Girls. For the better, that is. This song is nice and sweet, not loud  and obnoxious like the above.9.) Freak out - I didn't realize it, but I  liked this song so much I was playing it over and over!10.) Like the  rose - This reminded me of Castles in the air, kind of the same essence, if  you know what I mean. It's not fast moving and loud.11.) Never Giving up  - Nice, and catchy.12.) Oh Mr. Postman - Blech. I didn't like it.\"]\n",
            "pos : torch.bool torch.Size([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "954yNI5suXIc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}